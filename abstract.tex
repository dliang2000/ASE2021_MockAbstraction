\begin{abstract}
	Unit testing is a widely used tool in modern software development processes. A well-known issue in writing tests is handling dependencies: creating usable objects for dependencies is often complicated. Developers must therefore often introduce mock objects to stand in for dependencies during testing. 

	We believe that the static analysis of test suites, alongside the systems under test, can enable developers to better characterize the behaviours of existing test suites, thus guiding further test suite analysis and manipulation. However, because mock objects are created using reflection, they confound existing static analysis techniques. At present, it is impossible to statically distinguish methods invoked on mock objects from methods invoked on real objects. Static analysis tools therefore currently cannot determine which dependencies' methods are actually tested, versus mock methods being called.

	In this paper, we introduce MockDetector, a technique to identify mock objects and pinpoint method invocations on mock objects. We built the analysis on two different implmentations: the first implementation follows the imperative dataflow analysis approach using Soot, whereas the second one utilizing the declarative dataflow analysis on Doop. The analysis is able to handle common Java mock libraries' APIs for creating mock objects and propagate this information inter-procedurally through test cases. %may remove the last sentence if inter-procedural analysis is not implemented by the deadline.
	Following our observations of tests in the wild, we have added special-case support for arrays and collections holding mock objects. For a wide range of benchmarks being analyzed, our imperative dataflow analysis approach reported 2,042 invocations on mock objects, whereas our declative dataflow approach reported 2,124 invocations on mock objects, out of a total number of 63,011 method invocations in test suites. This suggests that roughly 3.3\% of method invocations will no longer consider for method coverage in static analyses, therefore improving the precision of static analyses. %Both implementations have reported the same number of intra-procedural mock invocations on 4 out of the 8 open source benchmarks analyzed. 
	
	% results part to be updated
\end{abstract}
