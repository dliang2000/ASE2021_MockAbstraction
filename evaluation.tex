\section{Evaluation}
\label{sec:evaluation}

\begin{table*}
	\centering
	\caption{Our suite of 8 open-source benchmarks (8000--117000 LOC) plus our microbenchmark. Soot and Doop analysis runtimes.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrrr}
		\toprule
		Benchmark & Total LOC & Test LOC & \thead{Soot intraproc \\ total time (s)} & \thead{Doop intraproc \\ total time (s)} & \thead{Soot intraproc \\ mock analysis (s)}  & \thead{Doop intraproc \\ mock analysis (s)} & \thead{Doop interproc \\ mock analysis (s)} \\
		\midrule
		bootique-2.0.B1-bootique         &  15530   & 8595   & 58  & 2810  &  0.276   & 19.93  & 24.90    \\
		commons-collections4-4.4         &  65273   & 36318  & 114 & 694   &  0.386   & 14.20  & 16.64     \\
		flink-core-1.13.0-rc1            &  117310  & 49730  & 341 & 1847  &  0.415   & 27.21  & 62.12      \\
		jsonschema2pojo-core-1.1.1       &  8233    & 2885   & 313 & 1005   &  0.282   & 29.33 & 41.05      \\
		maven-core-3.8.1   		         &  38866   & 11104  & 183 & 588   &  0.276   & 19.49  & 23.42     \\
		micro-benchmark         		 &  954     & 883	& 47  & 387   &  0.130   & 11.73   & 12.92     \\
		mybatis-3.5.6         		  	 &  68268   & 46334  & 500 & 4477  &  0.662   & 59.83  & 192.16      \\
		quartz-core-2.3.1        	  	 &  35355   & 8423   & 155 & 736   &  0.231   & 21.06  & 21.92   \\
		vraptor-core-3.5.5         	  	 &  34244   & 20133  & 371 & 1469  &  0.455   & 34.95  & 149.38    \\
		\bottomrule
		Total         	  				 &  384033  & 184405 & 2082 & 14013 &  3.123  & 237.73  & 544.51    \\
	\end{tabular}}
	%	\end{adjustbox}
	\label{tab:runtimes}
\end{table*}

\begin{table*}
	\centering
	\caption{Counts of Test-Related (Test/Before/After) methods in public concrete test classes, along with counts of mocks, mock-containing arrays, and mock-containing collections, reported by Soot intraprocedural analysis.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\begin{tabular}{lrrrr}
		\toprule
		Benchmark & \thead{\# of Test-Related \\ Methods} & \thead{\# of Test-Related \\ Methods with \\ mocks (intra)}  & \thead{\# of Test-Related \\ Methods with \\ mock-containing\\ arrays (intra)} & \thead{\# of Test-Related \\ Methods with \\ mock-containing\\ collections (intra)} \\
		\midrule
		bootique-2.0.B1-bootique           		&  420        &  32  & 7 & 0       \\
		commons-collections4-4.4          		&  1152       &  3   & 1 & 1       \\
		flink-core-1.13.0-rc1           		&  1091       &  4   & 0 & 0       \\
		jsonschema2pojo-core-1.1.1           	&  145        &  76  & 1 & 0       \\
		maven-core-3.8.1	           			&  337        &  24  & 0 & 0       \\
		micro-benchmark         		  		&  59         &  43  & 7 & 25       \\
		mybatis-3.5.6         		  			&  1769       &  330 & 3 & 0       \\
		
		quartz-core-2.3.1         	  			&  218     	  &  7   & 0 & 0      \\
		vraptor-core-3.5.5         	  			&  1119       &  565 & 15 & 0      \\
		\bottomrule
		Total        	  						&  6310       &  1084  & 34 & 26    \\
	\end{tabular}
	%	\end{adjustbox}
	\label{tab:mocks}
\end{table*}

\begin{table*}
	\centering
	\caption{Comparison of Number of InstanceInvokeExprs on Mock objects analyzed by Soot and Doop, and Total Number of InstanceInvokeExprs, in each benchmark's test suite.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrr}
		\toprule
		Benchmark & \thead{Total Number \\ of Invocations} & \thead{Mock Invokes \\ intraproc (Soot)} & \thead{Basic-only, \\ intraproc (Doop)} & \thead{Context-insensitive, \\ intraproc (Doop)} &  \thead{Basic-only, \\ interproc (Doop)} &\thead{Context-insensitive, \\ interproc (Doop)} \\
		\midrule
		bootique-2.0.B1-bootique           		&  3366     &  99   & 99    & 99   & 120   & 122    \\
		commons-collections4-4.4       			&  12753    &  11   & 3     &  3   & 23    & 23   \\
		flink-core-1.13.0-rc1           		&  11923    &  40   & 40    & 40   & 1262  & 1389   \\
		jsonschema2pojo-core-1.1.1      	     	&  1896     &  276  & 282   & 282  & 462   & 604   \\
		maven-core-3.8.1           			&  4072     &  23   & 23    & 23   & 31    & 39  \\
		microbenchmark         		  		&  471      &  108  & 123   & 123  & 132   & 132   \\
		mybatis-3.5.6         		  		&  19232    &  575  & 577   & 577  & 644 & 1345     \\
		quartz-core-2.3.1       	  		&  3436     &  21   & 21    & 21   & 23    & 31    \\
		vraptor-core-3.5.51        	  		&  5868     &  942  & 963   & 962  & 1301  & 1630   \\
		\bottomrule
		Total        	  				&  63017    & 2095  & 2131   & 2130  & 3998  & 5315   \\
	\end{tabular}}
	%	\end{adjustbox}
	\label{tab:invokes}
\end{table*}

%\begin{table*}
%	\centering
%	\caption{Doop analysis-only runtime after basic-only and context-insensitive base analyses. N/A = timed out after 90 minutes.}
%	%	\begin{adjustbox}{width=0.1\textwidth}
%	\begin{tabular}{lrrrrrr}
%		\toprule
%		Benchmark & \thead{Basic-only, \\ intraproc (s)} & \thead{Context-insensitive, \\ intraproc (s)} & \thead{Basic-only, \\ interproc (s)}  & \thead{Context-insensitive, \\ interproc (s)}  \\
%		\midrule
%		bootique-2.0.B1-bootique           		& 15.71  & 16.81 &  24.26    &  20.20     \\
%		commons-collections4-4.4           		& 17.42  & 12.26 &  21.79    &  15.36        \\
%		flink-core-1.13.0-rc1           		& 24.67  & 25.30 &  71.67    &  66.10         \\
%		jsonschema2pojo-core-1.1.1         		& 25.98  & 26.27 &  42.14    &  39.21         \\
%		maven-core-3.8.1   		        	& 18.01  & 16.34 &  25.49    &  22.09          \\
%		micro-benchmark         			& 10.97  & 10.50 &  12.51    &  12.53        \\
%		mybatis-3.5.6         		  		&  N/A   & 51.25 &   N/A     & 183.86          \\
%		quartz-core-2.3.1        	  		& 17.72  & 19.83 &  22.99    &  21.14        \\
%		vraptor-core-3.5.5         	  		& 22.10  & 23.81 &  66.73    & 146.09       \\
%		\bottomrule
%	\end{tabular}
%	%	\end{adjustbox}
%	\label{tab:doop-runtimes}
%\end{table*}

\begin{table*}
	\centering
	\caption{Comparison of Statement Coverage and Branch Coverage with all test cases,\\ and with test cases excluding intraprocedural mock invocations.}
	\vspace*{.5em}
	\begin{tabular}{lrrrrr} \toprule
			& \multicolumn{2}{c}{Statement Coverage} & & \multicolumn{2}{c}{Branch Coverage} \\
			\cmidrule{2-3} \cmidrule{5-6}
			\thead{Benchmark} & \thead{All Test Cases} & \thead{Test Cases without \\ Intraproc Mocks} & & \thead{All Test Cases} & \thead{Test Cases without \\ Intraproc Mocks} \\ 
			\midrule
			
			jsonschema2pojo-core-1.1.1  & 37\%  & 24\% & & 33\%    &  19\%     \\
			maven-core-3.8.1   		    & 48\%  & 48\% & & 39\%    &  38\%       \\
			mybatis-3.5.6   		    & 85\%  & 81\% & &  82\%    &  76\%        \\
			vraptor-core-3.5.5         	& 87\%  & 59\% & & 81\%   &  56\%    \\
			\bottomrule
	\end{tabular}
	\label{tab:test-coverages}
\end{table*}

\begin{table*}
	\centering
	\caption{Comparison of \% of test cases with reported focal methods by the two automated focal method detection algorithms.}
	\vspace*{.5em}
	\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrlrrrr} \toprule
		\multicolumn{5}{c}{Ghafari's algorithm} & & \multicolumn{5}{c}{methods2test}\\
		\cmidrule{1-5} \cmidrule{7-11}
		\thead{Benchmark} & \thead{Source Code \\ KLoC} & \thead{Reported \\ Focal Methods} & \thead{Test Cases} & \thead{\% of test cases \\ with focal \\ methods detected} &  & \thead{Benchmark} & \thead{Source Code \\ KLoC} & \thead{Reported \\ Focal Methods} & \thead{Test Cases} & \thead{\% of test cases \\ with focal \\ methods detected} \\ 
		\cmidrule{1-5} \cmidrule{7-11}
		
		commons-email-1.3.3 & 8.78 & 90  & 130 &  69\%  &  &    goja-0.1.14/goja-core  & 11.52 & 27  & 80 &  34\% \\
		PureMVC-1.0.8 & 19.46 & 34  & 43 &  79\%  &  &  mock-socket-0.9.0    & 1.09  & 4  & 34 &  12\%      \\
		XStream-1.4.4 & 54.93 & 513  & 968 &  53\%   &  &  project-sunbird-4.3.0/sunbird-lms-service   & 45.36 & 310  & 984 &  31\%   \\
		JGAP-3.4.4  & 73.96 & 1015  & 1390 &  73\%  &  &   optiq-0.8/core    & 93.94  & 26  & 1346 &  2\%   \\
		\bottomrule
		Geometric Mean &   &  &  &  68\% &   &  &  &  &  &  12\% 
	\end{tabular}}
	\label{tab:focal-method-algorithm-comparison}
\end{table*}

%\begin{table*}
%	\centering
%	\caption{Counts of mock invocations for Doop in basic-only and context-insensitive options, and for inter-procedural and intra-procedural .}
%	%	\begin{adjustbox}{width=0.1\textwidth}
%	\begin{tabular}{lrrrr}
%		\toprule
%		Benchmark & \thead{Basic-only, \\ intraproc} & \thead{Context-insensitive, \\ intraproc} & \thead{Basic-only, \\ interproc} & \thead{Context-insensitive, \\ interproc} \\
%		\midrule
%		bootique-2.0.B1-bootique           		&    &    &   &       \\
%		commons-collections4-4.4           		&    &    &   &        \\
%		flink-core-1.13.0-rc1           		&    &    &   &         \\
%		jsonschema2pojo-core-1.1.1         		&    &    &   &          \\
%		maven-core-3.8.1   		           		&    &    &   &           \\
%		micro-benchmark         		  		&    & 	  &   &           \\
%		mybatis-3.5.6         		  			&    &    &   &          \\
%		quartz-core-2.3.1        	  			&    &    &   &         \\
%		vraptor-core-3.5.5         	  			&    &    &   &         \\
%		\bottomrule
%		Total         	  						&    &    &   &         \\
%	\end{tabular}
%	%	\end{adjustbox}
%	\label{tab:doop-mock-invokes}
%\end{table*}

%% The goal of our study is to correctly identify and trace mock objects as well as the method invocations in the test suite. To this end, we conduct quantitative and qualitative research focusing on two research questions:

%% \begin{quote}
%% 	\emph{RQ 1: Are the mocks correctly identified and traced for each test method?}
%% \end{quote}

%% \begin{quote}
%% 	\emph{RQ 2: Would this be helpful for existing static analysis tools?}
%% \end{quote}

%% \subsection{Quantitative Analysis}
%% \label{subsec:effectiveness}

We have evaluated \textsc{MockDetector} on 8 open-source benchmarks, along with a micro-benchmark that we developed to test our tool. We ran all of our experiments on a 32-core Intel(R) Xeon(R) CPU E5-4620 v2 at 2.60GHz with 128GB of RAM running Ubuntu 16.04.7 LTS.

Table~\ref{tab:runtimes} presents summary information about our benchmarks and runtimes, namely the LOC and Soot and Doop analysis runtimes for each benchmark. The 9 benchmarks include over 383 kLOC, with 184 kLOC in the test suites, as measured by SLOCCount. The Soot total time is the amount of time that it takes for Soot to analyze the benchmark and test suite in whole-program mode, including our analyses. The Soot intra-procedural analysis time is the sum of runtimes for the main analysis plus two pre-analyses, as described in Section~\ref{subsec:soot}. Meanwhile, the reported Doop runtime is from the context-insensitive analysis, while the Doop analysis time for intra-procedural mock invocation analysis is for running the analysis alone based on recorded facts from the benchmark. The total Doop runtime is much slower than the total Soot runtime because Doop always computes a callgraph, which is an expensive operation. The Doop analysis-only time is also slower than the Soot time; we believe that this is because it computes a solution over the entire program, as opposed to Soot, which works one method at a time.
%The major difference between the Doop's total runtime and the actual time spent on mock invocation analysis comes from the build of the complete graph. %Add reference for SLOCCount.

We next investigated the prevalence of mocks. Table~\ref{tab:mocks} presents the number of test-related (Test/Before/After) methods which contain local variables or which access fields that are mocks, mock-containing arrays, or mock-containing collections, as reported by our Soot-based intra-procedural analysis. Across the 8 benchmarks, test-related methods containing local/field mocks or mock-containing containers accounted for 0.35\% to 51.8\% of the total number of test-related methods found in public concrete test classes. Our benchmarks are from different domains and created by different groups of developers. The difference in mock usage reflects their different philosophies and constraints regarding the creation and usage of mock objects in tests. Benchmarks like \textsc{vraptor-core} and \textsc{jsonschema2pojo-core} have more than half of their test-related methods containing mock objects (and mock-containing arrays); in both of these, most field mocks are created via annotations and reused in multiple test cases in the same class.

The core result is in Table~\ref{tab:invokes}, which presents the number of method invocations on mocks detected by our implementations. We present numbers from the imperative intraprocedural Soot implementation, as well as four versions of the declarative Doop implementation\footnote{mybatis times out for Doop's basic-only analysis even without our mock analysis, and we report N/A for its times; surprisingly, Doop context-insensitive successfully analyzes it.}: \{ ``basic-only'' (class hierarchy analysis, ``context-insensitive'' \} Doop base analysis $\times$ \{ intraprocedural, interprocedural \}. Note that our declarative and imperative implementations find exactly the same number of intraprocedural mock invocations for 4 of our benchmarks. On the others, the main source of missed mock calls in the Soot implementation is from missing support for array or collection related functions. Our intra-procedural analysis finds that method invocations on mock objects account for a range from 0.086\% to 16.4\% of the total number of invocations. 

%--- discuss the numbers for the interprocedural analysis.

In Section~\ref{sec:common} we discussed the implementation of our intraprocedural and interprocedural analyses. We can now discuss the effects of these implementation choices on the experimental results. Recall that we chose, unsoundly, to not propagate any information across method calls in the intraprocedural analysis. Thus, the intraproc columns in Table~\ref{tab:invokes} show smaller numbers than the interproc columns, as expected. The minor difference in \textsc{vraptor-core} is due to one method (which ought to be present) not showing up in Doop's context-insensitive callgraph. Note also that there is a sometimes drastic increase from the intraprocedural to the interprocedural result, e.g. from 40 to 1300 for \textsc{flink}. This is because mocks can (especially context-insensitively) propagate from tests to the methods that they call and all around the main program code. It would be desirable to be able to differentiate test helper methods, which we do want to propagate mocks to, from methods in the main program, which we generally do not want to propagate mocks to; however, our current analysis infrastructure treats test and main code identically.

We have successfully executed our Doop analysis with different base analyses. We chose to report numbers for the context-insensitive base analysis here as it matches our own analysis. (It would also be possible, but require significantly more effort, to adapt our analysis to carry around context.)

Talk about the new tables V and VI.

%--- , indicates the removal of mock invocations from call graph would improve the call graph's accuracy on method coverage for the benchmarks on the high end of the mock invocation percentage. 

%We explored the performance of our 4 declarative analysis variants based on recorded program facts, and present the numbers in Table~\ref{tab:doop-runtimes}. We can observe that the number of mock invocations correlates with the runtime; taking a bit more effort to compute a better call graph may well pay off in terms of overall analysis time. We suspect that the interprocedural analysis is especially slow for mybatis because we also analyze its 50 dependencies; that count is at the high end among our benchmarks.


%% \subsection{Application}
%% \label{subsec:static}

%% There are more test cases holding inter-procedural mocks (i.e., the mock object is created in a helper method and passed into the test case) in commons-collections and micro-benchmark. The inter-procedural analysis is currently in development and will be discussed in Section~\ref{sec:discussion}.

%% The Procedure Summaries produced after the analysis has indicated that the tracing of "mockiness" of variables and containers is also correct through the whole program. 

%% The accuracy results in tracing intra-procedural mock objects or containers have indicated that \textsc{MockDetector} has the potential to be applied as a helper for existing static analysis tools. By adding proper adjustment, it could pass the mock information to the static analysis, so that the generated call graph may appropriately omit the methods invoked on mock objects, thus increasing its accuracy.

%% By running evaluation (also inter-procedurally) on more benchmarks, our tool would have the potential to finding the scenario where developers prefer using mock objects for dependencies, and subsequently providing mock suggestions.


%% context-sensitivity for analysis
