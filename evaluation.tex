\section{Evaluation}
\label{sec:evaluation}

\begin{table*}
	\centering
	\caption{LOC and Runtime information for each benchmark.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\begin{tabular}{lrrrrrr}
		\toprule
		Benchmark & Total LOC & Test Suite LOC & \thead{Soot Intra-proc \\ Total Time (s)} & \thead{Doop Intra-proc \\ Total Time (s)} & \thead{Soot Intra-proc \\ Mock Analysis (s)}  & \thead{Doop Intra-proc \\ Mock Analysis (s)} \\
		\midrule
		bootique-2.0.B1-bootique           		&  15530   & 8595   &  &  &  0.293   & 86    \\
		commons-collections4-4.4           		&  65273   & 36318  &  &  &  0.345   & 53        \\
		flink-core-1.13.0-rc1           		&  117310  & 49730  &  &  &  0.358   & 194        \\
		jsonschema2pojo-core-1.1.1         		&  8233    & 2885   &  &  &  0.282   & 180       \\
		maven-core-3.8.1   		           		&  38866   & 11104  &  &  &  0.192   & 126        \\
		micro-benchmark         		  		&  954     & 883	&  &  &  0.126   & 24        \\
		mybatis-3.5.6         		  			&  68268   & 46334  &  &  &  0.524   & 344        \\
		quartz-core-2.3.1        	  			&  35355   & 8423   &  &  &  0.215   & 40      \\
		vraptor-core-3.5.5         	  			&  34244   & 20133  &  &  &  0.340   & 251      \\
		\bottomrule
		Total         	  						&  384033  & 184405 &  &  &  2.675   & 1298      \\
	\end{tabular}
	%	\end{adjustbox}
	\label{tab:runtimes}
\end{table*}

\begin{table*}
	\centering
	\caption{Counts of Test/Before/After methods in test suite containing MayMock object, ArrayMock containers, and CollectionMock containers in Test/Before/After methods. (Intra-procedurally)}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\begin{tabular}{lrrrr}
		\toprule
		Benchmark & \thead{\# of Test Related \\ Methods Invoked} & \thead{\# of Test Related \\ Methods with \\ MayMock (Intra)}  & \thead{\# of Test Related \\ Methods with \\ ArrayMock (Intra)} & \thead{\# of Test Related \\ Methods with \\ CollectionMock (Intra)} \\
		\midrule
		bootique-2.0.B1-bootique           		&  420        &  32  & 7 & 0       \\
		commons-collections4-4.4          		&  1152       &  3   & 1 & 1       \\
		flink-core-1.13.0-rc1           		&  1091       &  4   & 0 & 0       \\
		jsonschema2pojo-core-1.1.1           	&  145        &  76  & 1 & 0       \\
		maven-core-3.8.1	           			&  337        &  24  & 0 & 0       \\
		micro-benchmark         		  		&  59         &  43  & 7 & 25       \\
		mybatis-3.5.6         		  			&  1769       &  330 & 3 & 0       \\
		
		quartz-core-2.3.1         	  			&  218     	  &  7   & 0 & 0      \\
		vraptor-core-3.5.5         	  			&  1119       &  565 & 15 & 0      \\
		\bottomrule
		Total        	  						&  6310       &  1084  & 34 & 26    \\
	\end{tabular}
	%	\end{adjustbox}
	\label{tab:mocks}
\end{table*}

\begin{table*}
	\centering
	\caption{Comparison of Number of InstanceInvokeExprs on Mock objects analyzed by Soot and Doop, and Total Number of InstanceInvokeExprs, in each benchmark's test suite.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\begin{tabular}{lrrrrrr}
		\toprule
		Benchmark & \thead{Total Number \\ of Invocations} & \thead{Mock Invokes \\ Intra-proc(Soot)} & \thead{Basic-only, \\ intraproc} & \thead{Context-insensitive, \\ intraproc} & \thead{Basic-only, \\ interproc} & \thead{Context-insensitive, \\ interproc} \\
		\midrule
		bootique-2.0.B1-bootique           		&  3366     &  99  & 99  &   &   &    \\
		commons-collections4-4.4       			&  12753    &  11  & 3   &   &   &    \\
		flink-core-1.13.0-rc1           		&  11923    &  40  & 40  &   &   &    \\
		jsonschema2pojo-core-1.1.1           	&  1896     &  276  & 282   &   &   &    \\
		maven-core-3.8.1           				&  4072     &  23   & 23    &   &   &    \\
		microbenchmark         		  			&  471      &  108  & 123   &   &   &     \\
		mybatis-3.5.6         		  			&  19232    &  575  & 577   &   &   &       \\
		quartz-core-2.3.1       	  			&  3436     &  21   & 21    &   &   &     \\
		vraptor-core-3.5.51        	  			&  5868     &  942  & 962   &   &   &     \\
		\bottomrule
		Total        	  						&  63017    & 2095  & 2125   &   &   &    \\
	\end{tabular}
	%	\end{adjustbox}
	\label{tab:invokes}
\end{table*}

\begin{table*}
	\centering
	\caption{Doop runtime with basic-only and context-insensitive options.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\begin{tabular}{lrrrrrr}
		\toprule
		Benchmark & \thead{Basic-only, \\ no-mock (s)} & \thead{Basic-only, \\ interproc-mock (s)} & \thead{Basic-only, \\ delta (s)} & \thead{Context-insensitive, \\ no-mock (s)}  & \thead{Context-insensitive, \\ interproc-mock (s)} & \thead{Context-insensitive, \\ delta (s)} \\
		\midrule
		bootique-2.0.B1-bootique           		&    &    &   &   &    &     \\
		commons-collections4-4.4           		&    &    &   &   &    &         \\
		flink-core-1.13.0-rc1           		&    &    &   &   &    &         \\
		jsonschema2pojo-core-1.1.1         		&    &    &   &   &    &        \\
		maven-core-3.8.1   		           		&    &    &   &   &    &         \\
		micro-benchmark         		  		&    & 	  &   &   &    &         \\
		mybatis-3.5.6         		  			&    &    &   &   &    &         \\
		quartz-core-2.3.1        	  			&    &    &   &   &    &       \\
		vraptor-core-3.5.5         	  			&    &    &   &   &    &      \\
		\bottomrule
		Total         	  						&    &    &   &   &    &       \\
	\end{tabular}
	%	\end{adjustbox}
	\label{tab:doop-runtimes}
\end{table*}

%\begin{table*}
%	\centering
%	\caption{Counts of mock invocations for Doop in basic-only and context-insensitive options, and for inter-procedural and intra-procedural .}
%	%	\begin{adjustbox}{width=0.1\textwidth}
%	\begin{tabular}{lrrrr}
%		\toprule
%		Benchmark & \thead{Basic-only, \\ intraproc} & \thead{Context-insensitive, \\ intraproc} & \thead{Basic-only, \\ interproc} & \thead{Context-insensitive, \\ interproc} \\
%		\midrule
%		bootique-2.0.B1-bootique           		&    &    &   &       \\
%		commons-collections4-4.4           		&    &    &   &        \\
%		flink-core-1.13.0-rc1           		&    &    &   &         \\
%		jsonschema2pojo-core-1.1.1         		&    &    &   &          \\
%		maven-core-3.8.1   		           		&    &    &   &           \\
%		micro-benchmark         		  		&    & 	  &   &           \\
%		mybatis-3.5.6         		  			&    &    &   &          \\
%		quartz-core-2.3.1        	  			&    &    &   &         \\
%		vraptor-core-3.5.5         	  			&    &    &   &         \\
%		\bottomrule
%		Total         	  						&    &    &   &         \\
%	\end{tabular}
%	%	\end{adjustbox}
%	\label{tab:doop-mock-invokes}
%\end{table*}

The goal of our study is to correctly identify and trace mock objects as well as the method invocations in the test suite. To this end, we conduct quantitative and qualitative research focusing on two research questions:

\begin{quote}
	\emph{RQ 1: Are the mocks correctly identified and traced for each test method?}
\end{quote}

\begin{quote}
	\emph{RQ 2: Would this be helpful for existing static analysis tools?}
\end{quote}

\subsection{Quantitative Analysis}
\label{subsec:effectiveness}

We have evaluated \textsc{MockDetector} on 8 open-source benchmarks, along with a micro-benchmark that we developed to test our tool. Table~\ref{tab:runtimes} presents the LOC and Soot and Doop analysis runtimes for each benchmark. The 9 benchmarks include over 383 kLOC, with 184 kLOC in the test suites, as measured by SLOCCount. The Soot total time is the amount of time that it takes for Soot to analyze the benchmark and test suite in whole-program mode, including our analyses. The Soot intra-procedural analysis time is the sum of runtimes for the main analysis plus two pre-analyses, as described in Section~\ref{subsec:soot}. Meanwhile, the reported Doop runtime is from the context-insensitive analysis, while the Doop analysis time for intra-procedural mock invocation analysis is for running the analysis alone based on recorded facts from the benchmark.
%The major difference between the Doop's total runtime and the actual time spent on mock invocation analysis comes from the build of the complete graph. %Add reference for SLOCCount.

\subsection{Application}
\label{subsec:static}

There are more test cases holding inter-procedural mocks (i.e., the mock object is created in a helper method and passed into the test case) in commons-collections and micro-benchmark. The inter-procedural analysis is currently in development and will be discussed in Section~\ref{sec:discussion}.

The Procedure Summaries produced after the analysis has indicated that the tracing of "mockiness" of variables and containers is also correct through the whole program. 

The accuracy results in tracing intra-procedural mock objects or containers have indicated that \textsc{MockDetector} has the potential to be applied as a helper for existing static analysis tools. By adding proper adjustment, it could pass the mock information to the static analysis, so that the generated call graph may appropriately omit the methods invoked on mock objects, thus increasing its accuracy.

By running evaluation (also inter-procedurally) on more benchmarks, our tool would have the potential to finding the scenario where developers prefer using mock objects for dependencies, and subsequently providing mock suggestions.
