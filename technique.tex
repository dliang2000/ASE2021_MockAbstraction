\section{Technique}
\label{sec:technique}

We present two complementary ways of statically computing mock information: an imperative implementation of a dataflow analysis (using the Soot~\cite{Vallee-Rai:1999:SJB:781995.782008} program analysis framework), along with a declarative implementation (using the Doop~\cite{bravenboer09:_stric_declar_specif_sophis_point_analy} framework). We started this project with the usual imperative approach to implementing a static analysis---in our context, that meant using Soot. Then, when we wanted to experiment with adding more features to the analysis, we decided that this was a good opportunity to learn about Doop's declarative approach as well. We added new features to the Doop implementation and backported them to the Soot implementation. While the core analysis is similar, the different implementation technologies have different affordances. For instance, it is easier for the Doop version to mark a field as mock-containing than for the Soot version to do so. We start by describing each implementation in turn, and conclude this section with the commonalities between the two implementations. Section~\ref{sec:evaluation} then compares the results obtained using each technology.

\subsection{Imperative Soot Implementation}
\label{subsec:soot}
We first describe the Soot-based imperative dataflow analysis to find mock invocation sites. Our tool tracks information from the creation sites through the control-flow graph using a forward dataflow may-analysis---an object is declared a mock if there exists some execution path where it may receive a mock value. Our implementation also understands containers like arrays and collections, and tracks whether containers holds any mock objects. The abstraction marks all the contents of an array or container as potential mocks if it observes any mock object being put into the array or container.

% Mock Libraries discussed in Common Infrastructure section
%\paragraph{Define Common Mocking Library APIs}
%\label{subsubsec:collection}

%Our tool stores a pool of common APIs, provided by the analysis designer, which are used to create mock objects when using popular Java mocking libraries, including Mockito, EasyMock, and PowerMock. These APIs are the possible mock creation sites, where the locals/variables holding mock objects are first created.

%Given a pool of possible APIs to search for, our tool may analyze tests for their usage of these APIs.% Facts for mock source methods is discussed in doop, maybe this part could be discussed once to avoid repetition?


\paragraph{Forward Dataflow May Analysis}
\label{subsubsec:forward}

%To solve the problem, our tool uses forward may analysis, where it analyzes statements from top to bottom, and to keep variables that are verified to be mocks on any possible path at merged points. \textsc{MockDetector} uses the abstraction 
Our forward dataflow analysis maps values (locals and field references) in the Jimple intermediate representation to our abstraction:
\[ \mathtt{Value} \mapsto \mathtt{MockStatus}. \]
\texttt{MockStatus} records three bits: one for the value being a mock, one for it being an array containing a mock, and one for it being a collection containing a mock. At most one of the three bits may be true for any given value. Not having a mapping in the abstraction is equivalent to mapping to a MockStatus having all three bits false. 
%Our tool would store a value with MockStatus holding three false bits into the abstraction if and only if the value was once a mock or mock-containing container, but later redefined to a non-mock object or container without mock objects.

We chose to implement a may-analysis rather than a must-analysis for two reasons: 1) we did not observe any cases where a value was assigned a mock on one branch and a real object on the other branch; 2) implementing a must-analysis would not help heuristics to find focal methods, as a must-analysis would rule out fewer mock invocations. Our merge operation is therefore a fairly standard pointwise disjunction of the two incoming values in terms of values and in terms of the 3 bits of \texttt{MockStatus}.

%% \textsc{MockDetector} implements the "may" logic in the following manner: it checks the two in-flows 
%% of \begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
%% Map<Value, MockStatus>
%% \end{lstlisting}
%% from two paths. For any variable that is only stored in one map, the key-value pair is directly passed to the out-flow map. For a variable that is a shared key of the two maps, the analysis would update the out-flow's MockStatus by applying the "OR" operation on the "May Mock", "Array Mock", and "Collection Mock" bits from the MockStatus value retrieved from both in-flow maps. 

%% For each statement in a forward flow analysis, we consider two sets: generated set and killed set. In this study, the first set contains the locals that are judged to become mocks, whereas the killed set containing locals that are determined to no longer to be mocks. Equations (1) and (2) illustrates how the inflow and outflow are defined and calculated for each unit: $In(u)$, representing a program point before executing $u$, is the intersection of all outflows after executing each element in immediate predecessor statements of $u$; $Out(u)$, on the other hand, is determined by first removing the killed set from $In(u)$, and union the result with generated set. 

%% \begin{equation}
%% \mathrm{In}(u) = \bigcap_{u' \in preds(u)} \mathrm{Out}(u') 
%% \end{equation}

%% \begin{equation}
%% \mathrm{Out}(u) = (\mathrm{In}(u) - \mathrm{Kill}(u)) \bigcup \mathrm{Gen}(u) 
%% \end{equation}

Our dataflow analysis uses fairly standard gen and kill sets in the flow function. We set bits in \texttt{MockStatus} in the following cases:
%For the gen set, we consider for the Values in the following scenarios to be included in the abstraction, with mock bit set to 1:

First, the gen set includes pre-analyzed fields containing mock objects defined via annotation (e.g. \texttt{@Mock}), inside a constructor \texttt{<init>}, or in JUnit's \texttt{@Before}/\texttt{setUp()} methods. We discuss the pre-analysis below in Section~\ref{subsubsec:pre-analysis}. 

Second, it includes local variables assigned from mock-creation source methods:
\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
    X x = mock(X);
\end{lstlisting}
Third, it includes values assigned from return values of read methods from mock-containing collections or their array counterparts:
\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
    // array read;
    // r1 is in the in set as an array mock
    X x = r1[0];
    // collection read;
    // r2 is in the in set as a collection mock
    X x = r2.get(0);
\end{lstlisting}
Fourth, if \texttt{x} is a mock and casted and assigned to \texttt{x\_cast}, then the gen set includes \texttt{x\_cast} (e.g. \texttt{r1} in Listing~\ref{lis:arrayIllustrationIR}):
\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
    // x is in the in set as a mock
    X x_cast = (X) x;
\end{lstlisting}
Finally, the gen set includes copies of already-flagged mocks:
\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
    // x is in the in set with mock bit on
    X y = x;
\end{lstlisting}
The copy-related rules also apply to mock-containing arrays and collections, but we add some additional rules for generating mocks that the program reads from collections and arrays, as well as rules for marking arrays and collections as mock-containing.

For instance, in the below example, \texttt{r1} will be included in the gen set as an array mock, since it invokes an array write method, and \texttt{r2} (to be stored in the array) is a mock object as indicated in the in set. Similarly, \texttt{\$r4} will be added to the gen set as a collection mock, as it invokes an \texttt{ArrayList}'s write method, and \texttt{r3}, the object added to the ArrayList, is a known mock object.
\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
    // r2 is in the in set as a mock
    r1[0] = r2;
    // r3 is in the in set as a mock
    $r4.<java.util.ArrayList: boolean
             add(java.lang.Object)>(r3);
\end{lstlisting}

% nah, we say that above now.
%% In a similar fashion, the gen set will include values that traverse the program's control-flow graph via assignments, from a value that already has mock-containing array or mock-containing collection bit set to true.

%% \begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
%% // arr2 is already in the gen set 
%% // with mock-containing array bit on.
%% arr1 = arr2
%% // arr2 is already in the gen set 
%% // with collection containing mock bit on.
%% collection1 = collection2
%% \end{lstlisting}

%% ---------

%% In our analysis, the generated set consists of two steps. Consider the statement: 
%% \begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
%% Employee employee = mock(Employee.class);
%% \end{lstlisting}
%% The intermediate representation generated in Jimple format would be:
%% \begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
%% $r1 = staticinvoke <org.mockito.Mockito: 
%% java.lang.Object mock(java.lang.Class)>
%% (class "Lca/liang/Employee;")

%% r2 = (ca.liang.Employee) $r1
%% \end{lstlisting}W

%% In this example, $\$r1$ is the immediate receiver from Mockito's mock creation site, whereas $r2$ is the casted expression that gets carried along in the subsequent program. Thus, our tool would include the immediate receivers, and the casted expressions of mock objects into the generation set, in two steps. 

\paragraph{Interprocedural support} The Heros framework\cite{bodden12:_inter_proced_data_flow_analy} implements IFDS/IDE for program analysis frameworks including Soot. With some effort, it would be possible to rewrite our mock analysis with Heros; however, this would be a more involved process than in the declarative case, where we simply added two rules. In particular, Heros uses a different API in its implementation than Soot. However, conceptually, it should be no harder to implement an interprocedural Heros analysis than an intraprocedural Soot dataflow analysis.

\paragraph{Containers} Several test suites use arrays or collection objects to hold mock objects. In this scenario, our tool would consider that mockness propagates out to the container. For instance, say the flow function encounters an array write \begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
    r1[0] = r2
\end{lstlisting} 
Then, the tool will look for \texttt{r2} in the abstraction. Once the abstraction pinpoints \texttt{r2} (with mock bit on), it will include \texttt{r1} with mock-containing array bit set to true. (THIS PART STILL FEELS REPETITIVE.)


Taking an array as an example, our tool would first look for an array reference in the executing statement, meaning there is a read or a write from an array. If the effect is a STORE to the array, \textsc{MockDetector} would look for variables stored into the array, and check whether any of the variables have been found to be mocks. If so, it would label the array as an array mock, and set the relevant bit in the abstraction to true. Reversely, if is a LOAD effect from the array, \textsc{MockDetector} would check if the array itself has mock-containing array bit on in the abstraction. If so, it will mark the value assigned from the array LOAD with mock bit on, and store it in the abstraction.

*** we say more about collections in the related work, we should probably move that to here.

We treat collections analogously. The main difference is Java's \texttt{Collection} interface has multiple implementations, which expose different APIs for objects. \textsc{MockDetector} resolves this problem with a manually-constructed pool of read and write method APIs associated with each sub-type of the interface \texttt{java.util.Collection}. It subsequently checks (using the hierarchy) whether collection classes appear in statements containing invoke expression. This is achieved by first determining the declaring class of the invoked method. If the declaring class is of an interface, \textsc{MockDetector} would check whether \texttt{java.util.Collection} is a super-interface for the declaring class. Otherwise, if the declaring class is of a class type, \textsc{MockDetector} would check whether \texttt{java.util.Collection} is a super-interface for any of declaring class's implemented interfaces. If a collection sub-type container is presented, \textsc{MockDetector} would then check if a STORE effect is applied to the container, indicating some object is to be stored in the container. Once the object is determined to be a mock, the collection container variable would immediately be labelled as a collection mock, setting the relevant bit in the abstraction to true. Reversely, if a LOAD effect is applied to the container labelled as a collection mock, the object retrieved from the container will be labelled a mock, and setting the mock bit in the abstraction to true.

\subsubsection{Pre-Analysis for Field Mocks Defined in Constructors and Before Methods}
\label{subsubsec:pre-analysis} A number of our benchmarks define fields as mock objects via EasyMock or Mockito \texttt{@Mock} annotations, or initialize these fields in the \texttt{<init>} constructor or \texttt{@Before} methods (\textit{setUp()} in JUnit 3), which run before any test methods from those classes. These mock field or mock-containing container fields are then used in tests. In the Soot implementation, we use two pre-analyses before running the main analysis, under the assumption that fields are rarely, and not correctly, mutated in the test cases. (We have validated our assumption on benchmarks. Table~\ref{tab:mutations} shows a preliminary analysis of field mutation frequency inside test cases---fewer than 0.3\% of fields are mutated in test cases.)  

The first transformer handles annotated field mocks and field mocks defined in the constructors (\texttt{<init>} methods), while the second transformer handles \texttt{@Before} and \texttt{setUp()} methods. These transformers are implemented similarly.

\textsc{MockDetector} retrieves all fields in all test classes, and marks fields annotated {\tt Lorg/mockito/Mock;} or {\tt Lorg/easymock/Mock;} to its set of annotated mocks. \textsc{MockDetector} will store the three fields with \texttt{@Mock} into a HashSet, which later to be used in the main analysis.
% obvious enough that we don't need to belabour this point
% Listing~\ref{lis:annotatedMock} illustrates a Mockito annotated field mock example taken from \texttt{DefaultToolchainManagerTest.java} class in maven-core.

Listing~\ref{lis:fieldMock} depicts an example where instance fields are initialized using field initializers. Java copies such initializers into all class constructors (\texttt{<init>}). To detect such mock-containing fields, we simply apply the forward dataflow analysis on all constructors in the test classes prior to running the main analysis, using the same logic that we use to detect mock objects or mock-containing containers in the main analysis. The second pre-analysis transformer handles field mocks defined in \texttt{@Before} methods just like the first pre-analysis transformer handled constructors.

%% Listing~\ref{lis:fieldMock2} illustrates an example where fields are defined as mocks via mock-creation source methods inside the @Before method. Similarly, the field mocks initialized inside the @Before methods are determined by applying the forward dataflow analysis strictly on all @Before methods before the main analysis, where the values  


%% \begin{lstlisting}[basicstyle=\ttfamily, caption={Example for Annotated field mocks from \texttt{DefaultToolchainManagerTest.java} in maven-core.},
%% basicstyle=\scriptsize\ttfamily,language = Java, framesep=4.5mm,
%% framexleftmargin=1mm, captionpos=b, label=lis:annotatedMock]
%% public class DefaultToolchainManagerTest
%% {
%%     @Mock
%%     private Logger logger;
%%     @Mock
%%     private ToolchainFactory toolchainFactory_basicType;
%%     @Mock
%%     private ToolchainFactory toolchainFactory_rareType;
    
%%     @Before
%%     public void onSetup() throws Exception
%%     {    
%%         // ...
%%         MockitoAnnotations.initMocks( this );
%%         // ...
%%     }
%% }
%% \end{lstlisting}

\begin{lstlisting}[basicstyle=\ttfamily, caption={Example for field mocks defined by field initializations from \texttt{TypeRuleTest.java} in jsonschema2pojo.},
basicstyle=\scriptsize\ttfamily,language = Java, framesep=4.5mm,
framexleftmargin=1mm, captionpos=b, label=lis:fieldMock]
    private GenerationConfig config
                = mock(GenerationConfig.class);
    private RuleFactory ruleFactory
                = mock(RuleFactory.class);
    // ...
\end{lstlisting}

% not necessary here. you can add it to your thesis.
%% \begin{lstlisting}[basicstyle=\ttfamily, caption={Example for field mocks defined in a \texttt{@Before} method.},
%% basicstyle=\scriptsize\ttfamily,language = Java, framesep=4.5mm,
%% framexleftmargin=1mm, captionpos=b, label=lis:fieldMock2]
%% public class PayRollMockTest {
%%     private EmployeeDB employeeDB;
%%     private BankService bankService;
    
%%     @Before
%%     public void init() {
%%         // ...
%%         employeeDB = mock(EmployeeDB.class);
%%         bankService = mock(BankService.class);
%%         // ...
%%     }
%% }
%% \end{lstlisting}

\subsection{Declarative Doop Implementation}
We next describe the declarative Doop-based technique that \textsc{MockDetector} uses. Similarly to the dataflow analysis, the declarative approach propagates mockness from known mock sources, through the statements in the intermediate representation, to potential mock invocation sites.

% Mock Libraries discussed in Common Infrastructure section, perhaps refer to the paragraph in Common Infrastructure section?
The core of the implementation starts by declaring facts for 9 mock source methods manually gleaned from the mock libraries' documentation, as specified through method signatures (e.g. 
\texttt{<org.mockito.Mockito: java.lang.Object mock(java.lang.Class)>}.)
It then declares that a variable {\tt v} satisfies \verb+isMockVar(v)+ if it is assigned from the return value of a mock source, or otherwise traverses the program's interprocedural control-flow graph, through assignments, which may possibly flow through fields, collections, or arrays. Finally, an invocation site is a mock invocation if the receiver object {\tt v} satisfies \verb+isMockVar(v)+. Listing~\ref{lst:core} presents the core rules for {\tt isMockVar}.

\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none,label={lst:core},caption={Core rules for propagating mockness via predicate {\tt isMockVar}.}]
    .decl isMockVar(v: Var)
    isMockVar(v) :-
        AssignReturnValue(mi, v),
        callsMockSource(mi).
    isMockVar(v) :-
        isMockVar(from),
        AssignCast(_ /* type */, from,
                      v, _ /* inmethod */).
    isMockVar(v) :-
        isMockVar(v1),
        AssignLocal(v1, v, _).
\end{lstlisting}

We designed the analysis in a modular fashion, such that the interprocedural, collections, arrays, and fields support can all be disabled through the use of \verb+#ifdef+s, which can be specified on the Doop command-line.

\paragraph{Interprocedural support} From our perspective, including (context-insensitive) interprocedural support is almost trivial; we only need to add two rules
\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none]
isInterprocMockVar(v) :-
  AssignReturnValue(mi, v),
  mainAnalysis.CallGraphEdge(_, mi, _, callee),
  ReturnVar(v_callee, callee),
  isMockVar(v_callee).

isInterprocMockVar(v_callee) :-
  isMockVar(v),
  ActualParam(n, mi, v),
  FormalParam(n, callee, v_callee),
  mainAnalysis.CallGraphEdge(_, mi, _, callee),
  Method_DeclaringType(callee, callee_class),
  ApplicationClass(callee_class).
\end{lstlisting}
using call graph edges between the method invocation {\tt mi} and its callee {\tt callee}; the first rule propagates information from callees back to their callers, while the second rule propagates information from callers to callees through parameters. Note that we restrict our analysis to so-called ``application classes'', excluding in particular the Java standard library. We chose to run our conext-insensitive analysis on top of Doop's context-insensitive call graph, but have also reported results with Doop's \texttt{basic-only} analysis, which implements Class Hierarchy Analysis. Mirroring Doop, it would also be possible to add context sensitivity to our analysis, but our results suggest that this would not help much; we'll return to that point in Section~\ref{sec:evaluation}.

\paragraph{Arrays} Consistent with our analysis being a may-analysis, we use predicate {\tt isArrayLocalThatContainsMocks} to record local variables pointing to arrays that may contain mock objects. This predicate is true whenever the program under analysis stores a mock variable into an array; we also transfer array-mockness through assignments and casts. When a local variable is read from a mock-containing array, then the local variable itself is marked as a mock variable.

\paragraph{Fields} Apart from the obvious rule stating that a field which is assigned from a mock satisfies {\tt fieldContainsMock}, we also label fields that have the {\tt org.mockito.Mock} annotation as mock-containing. We declare that a given field \emph{signature} may contain a mock, i.e. the field with a given signature belonging to all objects of a given type.

\paragraph{Arrays and Fields} We also support not just array locals but also array fields. That is, when an array-typed field is assigned from a mock-containing array local, then it is also a mock. And when an array-typed local variable is assigned from a mock-containing array field, then that array local is a mock-containing array.

\paragraph{Containers} We support containers the same way we support arrays, except that we hardcode all relevant methods from the Java Collections API. There are 60 such methods in total, which together account for about 1/6th of our Doop analysis. In addition to straightforward get and put methods, we also support iterators, add-all methods, and collection copies via constructors. We also handle {\tt toArray} by propagating mockness from the collection to the array.

We also support containers stored in fields.

\subsection{Common Infrastructure}
We have parameterized our technique with respect to mocking libraries and have instantiated it with respect to the popular Java libraries Mockito\footnote{\url{https://site.mockito.org/}}, EasyMock\footnote{\url{https://easymock.org/}}, and PowerMock\footnote{\url{https://github.com/powermock/powermock}}. We also support different versions of JUnit\footnote{\url{https://junit.org}}: 3, and 4+. We discuss the parameterization in this subsection.

Both JUnit and mocking libraries rely heavily on reflection to work, and would normally pose problems for static analyses. In particular, the set of reachable test methods is enumerated at runtime, and the mock libraries create mock objects reflectively. Fortunately, their use of reflection is limited and stylized, and we have designed our analyses to soundly handle these libraries.

\paragraph{JUnit and Driver Generation}
JUnit tests are simply methods that developers write in test classes, appropriately annotated (in JUnit 3 by method name starting with ``test'', in 4+ by a \texttt{@Test} annotation). A JUnit test runner uses reflection to find tests. Out of the box, static analysis engines do not see tests as reachable code.

% what about hierarchical drivers?

Thus, to enable static analysis over a benchmark's test suite, our tool uses Soot to generate a driver class for each Java sub-package of the suite (e.g. \texttt{org.apache.ibatis.executor.statement}). In each of these sub-package driver classes, our tool creates a \textit{runall()} method, which invokes all methods within the sub-package that JUnit (either 3 or 4) considers to be public, as well as non-constructor test cases, all surrounded by calls to class-level init/setup and teardown methods. Concrete test methods are particularly easy to call from a driver, as they are specified to have no parameters and are not supposed to rely on any particular ordering. 
Our tool then creates a RootDriver class at the root package level, which invokes the \textit{runall()} method in each sub-package driver class, along with the Test/Before/After methods created for the classes located at the root level. The drivers that we generate also contain code to catch all checked exceptions declared to be thrown by the unit tests. Both our Soot and Doop implementations use the generated driver classes.

All static frameworks must somehow approximate the set of entry points as appropriate for their targets. For instance, the Wala framework~\cite{wala19:_t} also creates synthetic entry points, but it does this to perform pointer analysis on a program's main code rather than to enumerate the program's test cases.

\paragraph{Intraprocedural Analysis} The Soot analysis is intraprocedural and the Doop analysis has an intraprocedural version. In both of these cases, we make the unsound (but reasonable for our anticipated use case) assumption that mockness can be dropped between callers and callees: at method entry points, no parameters are assumed to be mocks, and at method returns, the returned object is never a test. Doop's interprocedural version drops this assumption and instead context-insensitively propagates information from callers to callees and back; we discuss the results of doing so in Section~\ref{sec:evaluation}.

Call graphs are useful to our analysis in two ways: first, because they come with entry points (which we approximate directly in our context, as explained above); and second, because they help identify calls to mock source methods. We assume that developers do not call inherited versions of mock creation sites (though again, if a call graph is available in Doop, we use it).

\paragraph{Mock Libraries}
Our supported mock libraries take different approaches to declaring mocks. All of the libraries have methods that generate mock objects; for instance, EasyMock contains the \texttt{createMock()} method. We consider return values from these methods to be mock objects. Additionally, Mockito contains a fluent \texttt{verify()} method which returns a mock object. Finally, Mockito also allows developers to mark fields as \texttt{@Mock}; we treat reads from such fields as mock objects. Both implementations start the analysis by using these hard coded these facts on mock source methods, as described in the mock libraries' documentation.
