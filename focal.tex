\section{Application: Finding Focal Methods}
\label{sec:focal}

We anticipate that one useful application of mock detection is to
improve the detection of focal methods in test cases, or as Ghafari et
al~\cite{ghafari15:_autom} call them, focal methods under test
(F-MUTs). In this section, we will discuss some approaches to finding
focal methods, and describe how our analysis can facilitate this search.

Ghafari's approach is the one that is most amenable to our work.
Their approach is applicable to tests for classes that implement
stateful objects, and assume that each test has at least one focal
method, seen as a call to a mutator method for the class under test.
The last such call is the focal method. However, their approach can
only work on tests of mutator methods.  Purely-functional methods, for
instance, are beyond the scope of their approach.

Another approach is name-based, as seen in the methods2test
dataset~\cite{tufano2020unit}.  This approach uses two heuristics: 1)
the name of the test matches the name of the focal method; or, 2) the
test calls a unique method in the focal class. Rompaey and
Demeyer~\cite{rompaey09:_estab_traceab_links_unit_test} explore the
name-based approach along with simply using the last method called---not necessarily a mutator---as the class containing the focal method.  Romaey and Demeyer also
explore other heuristics, as we discuss in~\ref{sec:related}. Ghafari et al point out
that these heuristics have significant limitations, especially when a
test case implements several sub-scenarios.

\todo{We discuss precision and recall issues.}

\paragraph{Precision and Recall}

Regarding the performance of the two approaches, we investigate their approximated precision and recall rate. Table~\ref{fig:mockExampleEvaluation} highlights the percentage of test cases each of the two approaches automatically report focal methods, which we treat this data as an approximated recall rate. The four projects presented under Ghafari's algorithm are directly cited from their paper, whereas we hand picked the four benchmarks from methods2test's dataset so that they have a similar number of test cases. 

The data suggests that methods2test's named-based approach of focal method detection has a quite low recall rate. This is not surprising as their heuristics have quite restricted constraints. In addition, we run mock analysis over methods2test's focal method data on some of the benchmarks evaluated in Section~\ref{sec:evaluation}, and confirm that none of the reported focal methods are mock invocations. This suggests methods2test's approach has a high precision in reporting focal methods.

On the other hand, we obtain the CSV dataset over some recent analyzed projects from Ghafari's team, and find out that there are a few types of unit test cases they have to manually report focal method (discussed in the next paragraph) to either fill in the empty automatic reporting, or to override the automatically detected focal method. Overall, Ghafari's algorithm requires more work to increase the precision but the percentage of test cases with focal methods automatically reported (we treat it as an approximated recall rate) is acceptable. 

In short, regarding the focal methods detection, methods2test's approach performs with a high precision but low recall, whereas Ghafari's algorithm has an acceptable recall but with low precision.

\paragraph{Example} We now revisit the unit test case presented in Listing~\ref{lis:mockCall}. Figure~\ref{fig:mockExampleEvaluation} highlights the process for finding the mock object \texttt{session} and consequently the mock invocation \texttt{getRequest()} in the unit test. Both Soot and Doop implementations report this mock invocation 
from the test case. Thus, with the assistance of \textsc{MockDetector}, \texttt{getRequest()} can be removed from consideration as a focal method. We judge \texttt{getToolchainsForType()} to be the focal method since it is the only method invocation remaining after the elimination process. In addition, for the return array \texttt{basics}, its \texttt{length} attribute indeed gets checked in the assertion statement on Line~\ref{line:assert}, which means this test case indeed tests the behaviour of \texttt{toolchainManager.getToolchainsForType()}. 

For this unit test, since Ghafari's algorithm does not consider accessors (or so-called inspector methods in the paper) as focal methods, they will presumably report no focal method for this unit test case. This clearly is an incorrect result from Ghafari's algorithm.
Incidentally, since the heuristic of Ghafari's algorithm requires the unit test case to have at least one assertion statement, the algorithm will presumably fail to return any focal methods when analyzing unit test cases without assertions.

As for the heuristica for methods2test, method \texttt{getToolChainsForType()} does not seem like it should match under the name-based heuristic from methods2test. The other heuristic finds calls to the focal class, so methods2test should properly identify the call to \texttt{getToolChainsForType()} using that heuristic. Our mock analysis can help with that heuristic as well, by removing mocks from the set of eligible focal classes.

\begin{table*}
	\centering
	\caption{Comparison of \% of test cases with reported focal methods by the two automated focal method detection algorithms.}
	\vspace*{.5em}
	\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrlrrrr} \toprule
		\multicolumn{5}{c}{Ghafari's algorithm} & & \multicolumn{5}{c}{methods2test}\\
		\cmidrule{1-5} \cmidrule{7-11}
		\thead{Benchmark} & \thead{Source Code \\ KLoC} & \thead{Reported \\ Focal Methods} & \thead{Test Cases} & \thead{\% of test cases \\ with focal \\ methods detected} &  & \thead{Benchmark} & \thead{Source Code \\ KLoC} & \thead{Reported \\ Focal Methods} & \thead{Test Cases} & \thead{\% of test cases \\ with focal \\ methods detected} \\ 
		\cmidrule{1-5} \cmidrule{7-11}
		
		commons-email-1.3.3 & 8.78 & 90  & 130 &  69\%  &  &    goja-0.1.14/goja-core  & 11.52 & 27  & 80 &  34\% \\
		PureMVC-1.0.8 & 19.46 & 34  & 43 &  79\%  &  &  mock-socket-0.9.0    & 1.09  & 4  & 34 &  12\%      \\
		XStream-1.4.4 & 54.93 & 513  & 968 &  53\%   &  &  project-sunbird-4.3.0/sunbird-lms-service   & 45.36 & 310  & 984 &  31\%   \\
		JGAP-3.4.4  & 73.96 & 1015  & 1390 &  73\%  &  &   optiq-0.8/core    & 93.94  & 26  & 1346 &  2\%   \\
		\bottomrule
		Geometric Mean &   &  &  &  68\% &   &  &  &  &  &  12\% 
	\end{tabular}}
	\label{tab:focal-method-algorithm-comparison}
\end{table*}


\begin{figure}[h]
	\begin{lstlisting}[
	basicstyle=\ttfamily\scriptsize,language = Java, framesep=4.5mm, framexleftmargin=1.0mm, captionpos=b, escapechar={|}, mathescape=true, morekeywords={@Test}]
  @Test
  public void testMisconfiguredToolchain() 
                      throws Exception {
    //                mock:|\cmark|         mockAPI:|\cmark|
    MavenSession |\colorbox{olive}{session}| = |\colorbox{teal}{mock}| ( MavenSession.class );
    MavenExecutionRequest req =
        new DefaultMavenExecutionRequest();
    //     mock invocation:|\cmark| $\Rightarrow$ focal method:|\xmark|
    when( session.|\colorbox{red}{getRequest()}| ).thenReturn( req ); |\label{line:Fmock}|

    ToolchainPrivate[] basics =
      //                          focal method:|\cmark|
      toolchainManager.|\colorbox{gray}{getToolchainsForType}|("basic", 
                                                           session); |\label{line:Freal}|

    assertEquals( 0, basics.length );|\label{line:assert}|
  }
  \end{lstlisting}

  \caption{Qualitative evaluation of removing mock invocation from focal method consideration.}
  \label{fig:mockExampleEvaluation}
\end{figure}
