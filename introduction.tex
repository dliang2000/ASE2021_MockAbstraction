\section{Introduction}
\label{sec:introduction}

Mock objects ~\cite{beck02:_test_driven_devel} are a common idiom in
unit tests for object-oriented systems.  They allow developers to test objects that 
rely on other objects, likely from different components, or are simply complicated 
to build for testing purposes (e.g. a database).

While mock objects are an invaluable tool for developers, their use
complicates the static analysis of test cases. 
By design, a call to a mock object resembles a call to the real object. 
A naive static analysis attempting to be sound will have to include all of 
the possible behaviours of the actual object when analysing such code. 
As a result, such analysis would be useless in capturing the true behaviour 
of the test case.

Others have defined the notion of a focal method for a test case---the method
whose behaviour is being tested. ~\cite{7335402}
The test case would set up mock objects to provide parameters to this focal method.
When analyzing the test case, it would be useful to know which variables in the
method contain mock objects, in order to build a more precise call graph and get 
better insight of the overall project.

We have designed a helper static analysis, \textsc{MockDetector}, which identifies
mock objects in test cases. It starts from a list of mock object creation sites; we
have provided APIs for the common mocking libraries. It then propagates the mockiness
intraprocedurally through a forward flow must analysis and to containers, so that an analysis
can ask whether a given variable in a test case contains a mock or not. Currently, we have
evaluated \textsc{MockDetector} on a suite of 3 benchmarks. Manual inspection have shown 
correct intraprocedural mock detection over the benchmarks. 

Taking a broader view, we believe that helper static analyses can aid
in the development of more useful static analyses. These analyses can
encode useful domain properties; for instance, in our case, properties
of test cases. By taking a domain-specific approach, analyses can extract
useful facts about programs that would otherwise be difficult to establish.

We make the following contributions in this paper:
\begin{itemize}
\item We designed and implemented two related static mock detection algorithms, one as a dataflow analysis implemented imperatively (using Soot) and the other declaratively (using Doop).
\item We evaluate the relative precision of the imperative and declarative approaches, with and without special-case support for containers and arrays. % potentially intraprocedural as well
\item Using our tool, we characterize the prevalence of mock objects in the test suites for a number of benchmarks, demonstrating one concrete application of our mock detection tool.
\end{itemize}
