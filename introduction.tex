\section{Introduction}
\label{sec:introduction}

Mock objects ~\cite{beck02:_test_driven_devel} are a common idiom in
unit tests for object-oriented systems.  They allow developers to test objects that 
rely on other objects, likely from different components, or are simply complicated 
to build for testing purposes (e.g. a database).

While mock objects are an invaluable tool for developers, their use
complicates the static analysis of test cases. 
By design, a call to a mock object resembles a call to the real object. 
A naive static analysis attempting to be sound will have to include all of 
the possible behaviours of the actual object when analysing such code. 
As a result, such analysis would be useless in capturing the true behaviour 
of the test case.

Others have defined the notion of a focal method for a test case---the method
whose behaviour is being tested. ~\cite{7335402}
The test case would set up mock objects to provide parameters to this focal method.
When analyzing the test case, it would be useful to know which variables in the
method contain mock objects, in order to build a more precise call graph and get 
better insight of the overall project.

We have designed a helper static analysis, \textsc{MockDetector}, which identifies
mock objects in test cases. It starts from a list of mock object creation sites; we
have provided APIs for the common mocking libraries. It then propagates mockness
intraprocedurally through a forward flow must analysis and to containers, so that an analysis
can ask whether a given variable in a test case contains a mock or not. Currently, we have
evaluated \textsc{MockDetector} on a suite of 3 benchmarks. Manual inspection have shown 
correct intraprocedural mock detection over the benchmarks. 

Taking a broader view, we believe that helper static analyses can aid
in the development of more useful static analyses. These analyses can
encode useful domain properties; for instance, in our case, properties
of test cases. By taking a domain-specific approach, analyses can extract
useful facts about programs that would otherwise be difficult to establish.

We make the following contributions in this paper:
\begin{itemize}
\item We designed and implemented two variants of a static mock detection algorithm, one as a dataflow analysis implemented imperatively (using Soot) and the other declaratively (using Doop).
\item We evaluate both the relative ease-of-implementation and precision of the imperative and declarative approaches, both intraprocedurally and interprocedurally. % potentially intraprocedural as well
\item We characterize our benchmark suite (8 open-source benchmarks, 184 kLOC) with respect to their use of mock objects, finding that 981 out of 6282 unit tests use intraprocedurally-detectable mock objects, and that there are a total of 1811 method invocations on mock objects. We further identify how powerful an analysis is required to identify mock object use---adding fields and collections adds X mock objects, while interprocedural techniques add Y mock objects.
\end{itemize}
At a higher level, we see this paper as making both a contribution and a meta-contribution to
problems in source code analysis. The contribution, mock detection, enables more accurate analyses
of test cases, which account for a signficant fraction of modern codebases. The meta-contribution,
comparing analysis approaches, will help future researchers decide how to best solve their
source code analysis problems. In brief, the declarative approach allows users to state properties more
concisely, while the imperative approach is more amenable to use in program transformation; we return
to this question in Section~\ref{sec:discussion}.

% also add somewhere:
% it's notoriously difficult to check static analyses but this is basically N-version programming and we can cross-check results.
